{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055ef41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7d8aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 2833 characters from the PDF.\n",
      "\n",
      "--- Extracted Text (Preview) ---\n",
      "Prabhav Sharma \n",
      "Delhi, New Delhi-12 | +91-7303025805 | prabhavs2004@gmail.com | GitHub | Portfolio | LinkedIn \n",
      "\n",
      "Summary \n",
      "Highly motivated Computer Science student with internships at Bharti Airtel, honing Python (Pandas, \n",
      "Scikit-learn) and SQL skills. Proven ability in data-driven problem-solving through Amazon hackathon \n",
      "participation. Adept at collaborating in teams. \n",
      "  Developing analytical models using Python (Pandas, Scikit-learn). \n",
      "\n",
      "  Executing complex database analyses via SQL. \n",
      "\n",
      "  Visualizing insights effectively with associated tools and utilizing Git, showcasing collaborative project \n",
      "\n",
      "management abilities. \n",
      "\n",
      "Skills \n",
      "• Data Analysis & Manipulation: Python Programming (Pandas, NumPy), Data Cleansing, ETL Process \n",
      "• Statistical Modeling: Classification Modeling, Regression Analysis, Machine Learning (Sklearn) \n",
      "• Data Visualization: Power BI, Tableau, Data Storytelling \n",
      "• Collaborative Technologies: Git/GitHub, Version Control, SQL Databases \n",
      "Projects \n",
      "Conversational Data Analytics Platform [Python, Pandas, SQL] \n",
      "  Developed a Streamlit application that transformed complex datasets into natural language insights for \n",
      "\n",
      "users. \n",
      "\n",
      "  Utilized Python libraries including Pandas for data manipulation and SQL for querying relational \n",
      "\n",
      "databases to leverage ternary data sources. \n",
      "\n",
      "  Achieved 70% faster data discovery and empowered users with actionable insights, leading to a 20% \n",
      "\n",
      "increase in analytics-driven decision-making. \n",
      "\n",
      "E-commerce Exploratory Data Analysis [Python, Scikit-learn, Matplotlib] \n",
      "  Conducted comprehensive exploratory data analysis to uncover patterns and anomalies in sales data \n",
      "\n",
      "for strategic business decisions. \n",
      "\n",
      "  Applied Python using Scikit-learn for predictive modeling and Matplotlib for creating visualizations \n",
      "\n",
      " \n",
      "\n",
      "that highlighted trends and operational efficiencies. \n",
      "Identified key performance indicators that improved forecast accuracy by 15% and reduced anomaly \n",
      "detection time by 25%. \n",
      "\n",
      "Predictive Diabetes Risk Stratification Model [Python, Numpy, Power BI] \n",
      "  Engineered and deployed a classification model to strategically predict diabetes risk, aiding healthcare \n",
      "\n",
      "providers in early interventions. \n",
      "\n",
      "  Employed Python with Numpy for data preprocessing and statistical analysis, integrating Power BI for \n",
      "\n",
      "interactive dashboards that enhanced data accessibility. \n",
      "\n",
      "  Drove a 10% reduction in misclassification rates and facilitated user adoption through intuitive \n",
      "\n",
      "reporting, impacting patient outcomes effectively. \n",
      "\n",
      "Education \n",
      "  PCM 12th | Gyan Mandir Public School| New Delhi | 2020-22 \n",
      "  B.Tech (CSE) | Jims Mangement Technical Campus, Greater Noida | 2022-26 \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fInternship & Training \n",
      "  Data Analyst Intern in Bharti Airtel (April 2025 – June 2025) \n",
      "  Data Science & Machine Learning (Udemy) \n",
      "  Data Analytics Program (Jobaaj Learning) \n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \n",
    "    try:\n",
    "        # extract_text is a convenient function from pdfminer.six\n",
    "        text = extract_text(pdf_path)\n",
    "        print(f\"Successfully extracted {len(text)} characters from the PDF.\")\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found at path: {pdf_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Usage Example ---\n",
    "# 1. IMPORTANT: Replace 'your_resume.pdf' with the actual path to your PDF file.\n",
    "pdf_file_path = r\"C:\\Users\\nt397\\Downloads\\Generated_Resume (1).pdf\"\n",
    "# 2. Call the function to load the PDF content\n",
    "resume_text = load_pdf_text(pdf_file_path)\n",
    "\n",
    "\n",
    "if resume_text:\n",
    "    print(\"\\n--- Extracted Text (Preview) ---\")\n",
    "    print(resume_text)\n",
    "    # The 'resume_text' variable now holds all the text from your PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e56422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nt397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nt397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Download the stop words list and the tokenizer data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf935139",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = \"\"\"\n",
    "Job Opening: Data Analyst\n",
    "\n",
    "Company: Quantiva Analytics Pvt. Ltd.\n",
    "Location: Gurgaon / Bangalore / Remote\n",
    "Employment Type: Full-Time\n",
    "\n",
    "Quantiva Analytics Pvt. Ltd. is a data-driven technology organization focused on business intelligence, reporting automation, and advanced analytics solutions. The company is looking to hire a Data Analyst who can convert raw data into meaningful insights to support decision-making across various business functions.\n",
    "\n",
    "Role Overview\n",
    "\n",
    "The selected candidate will be responsible for handling end-to-end analytical tasks, including data extraction, cleaning, preprocessing, visualization, and insight generation. The role requires strong analytical thinking, attention to detail, and the ability to collaborate with stakeholders to interpret business requirements and deliver data-backed recommendations.\n",
    "\n",
    "Key Responsibilities\n",
    "\n",
    "Extract, clean, and validate datasets from multiple internal and external sources.\n",
    "\n",
    "Conduct exploratory data analysis (EDA) to identify trends, patterns, and anomalies.\n",
    "\n",
    "Develop and maintain dashboards and automated reports using Power BI, Tableau, or Python visualization libraries.\n",
    "\n",
    "Write optimized SQL queries for data retrieval and transformation tasks.\n",
    "\n",
    "Collaborate with business and product teams to define analytical objectives and metrics.\n",
    "\n",
    "Present findings through structured reports and visual summaries.\n",
    "\n",
    "Support forecasting, KPI monitoring, and performance tracking initiatives.\n",
    "\n",
    "Required Technical Skills\n",
    "\n",
    "Strong command of SQL for querying and data manipulation.\n",
    "\n",
    "Proficiency in Python, especially using Pandas, NumPy, Matplotlib, and Seaborn.\n",
    "\n",
    "Experience with visualization tools such as Power BI, Tableau, or Google Data Studio.\n",
    "\n",
    "Knowledge of statistical concepts and analytical methods.\n",
    "\n",
    "Advanced Excel skills, including pivot tables and power query.\n",
    "\n",
    "Familiarity with Git/GitHub is preferred.\n",
    "\n",
    "Eligibility Criteria\n",
    "\n",
    "Degree: B.Tech/B.E., BCA, MCA, B.Sc, or M.Sc (any specialization).\n",
    "\n",
    "Graduation Year: 2025 or earlier.\n",
    "\n",
    "Strong analytical mindset with problem-solving skills.\n",
    "\n",
    "Good written and verbal communication abilities.\n",
    "\n",
    "Compensation\n",
    "\n",
    "Internship Stipend: ₹12,000 per month (if applicable).\n",
    "\n",
    "Duration: 3 months.\n",
    "\n",
    "Full-time role may be offered based on performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ad0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nt397\\AnacondaNew\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Similarity Score: 0.778251051902771\n",
      "The Candidate is eligible\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Clean text function (your cleaning code)\n",
    "def clean_and_remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.@]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens \n",
    "        if word not in stop_words and (len(word) > 1 or word in ['@', '.'])\n",
    "    ]\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "    clean_text = re.sub(r'\\s*@\\s*', '@', clean_text)\n",
    "    clean_text = re.sub(r'\\s*\\.\\s*', '.', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# 1. Clean your texts\n",
    "cleaned_jd_text = clean_and_remove_stopwords(jd_text)\n",
    "cleaned_resume_text = clean_and_remove_stopwords(resume_text)\n",
    "\n",
    "# 2. Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 3. Convert texts to embeddings\n",
    "jd_embedding = model.encode(cleaned_jd_text, convert_to_tensor=True)\n",
    "resume_embedding = model.encode(cleaned_resume_text, convert_to_tensor=True)\n",
    "\n",
    "# 4. Compute similarity\n",
    "similarity_score = util.cos_sim(jd_embedding, resume_embedding).item()\n",
    "print(\"Similarity Score:\", similarity_score)\n",
    "\n",
    "# 5. Decision based on score\n",
    "if similarity_score >= 0.60:\n",
    "    print(\"The Candidate is eligible\")\n",
    "\n",
    "elif 0.45 <= similarity_score < 0.60:\n",
    "    print(\"Needs manual review\")\n",
    "\n",
    "else:\n",
    "    print(\"The Candidate is not eligible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37ba2d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prabhav sharma delhi new delhi 12 91 7303025805 prabhavs2004@gmail.com github portfolio linkedin summary highly motivated computer science student internships bharti airtel honing python pandas scikit learn sql skills.proven ability data driven problem solving amazon hackathon participation.adept collaborating teams.developing analytical models using python pandas scikit learn.executing complex database analyses via sql.visualizing insights effectively associated tools utilizing git showcasing collaborative project management abilities.skills data analysis manipulation python programming pandas numpy data cleansing etl process statistical modeling classification modeling regression analysis machine learning sklearn data visualization power bi tableau data storytelling collaborative technologies git github version control sql databases projects conversational data analytics platform python pandas sql developed streamlit application transformed complex datasets natural language insights users.utilized python libraries including pandas data manipulation sql querying relational databases leverage ternary data sources.achieved 70 faster data discovery empowered users actionable insights leading 20 increase analytics driven decision making.commerce exploratory data analysis python scikit learn matplotlib conducted comprehensive exploratory data analysis uncover patterns anomalies sales data strategic business decisions.applied python using scikit learn predictive modeling matplotlib creating visualizations highlighted trends operational efficiencies.identified key performance indicators improved forecast accuracy 15 reduced anomaly detection time 25.predictive diabetes risk stratification model python numpy power bi engineered deployed classification model strategically predict diabetes risk aiding healthcare providers early interventions.employed python numpy data preprocessing statistical analysis integrating power bi interactive dashboards enhanced data accessibility.drove 10 reduction misclassification rates facilitated user adoption intuitive reporting impacting patient outcomes effectively.education pcm 12th gyan mandir public school new delhi 2020 22 b.tech cse jims mangement technical campus greater noida 2022 26 internship training data analyst intern bharti airtel april 2025 june 2025 data science machine learning udemy data analytics program jobaaj learning'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe85d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STRUCTURED RESUME DATA EXTRACTION\n",
      "==================================================\n",
      "\n",
      "--- Contact Information ---\n",
      "Phone: 7303025805\n",
      "Email: prabhavs2004@gmail.com\n",
      "\n",
      "--- Identified Skills ---\n",
      "Classification, Data Analytics, Data Science, Git, Github, Matplotlib, Numpy, Pandas, Power Bi, Python, Regression, Sql, Tableau\n",
      "\n",
      "--- Education/Certifications ---\n",
      "2020, 2022, April 2025 June 2025, B.Tech Cse, City Public School, Noida, Ducat, Noida Sec-63, Jims Management Technical Campus, Greater Noida\n",
      "\n",
      "--- Experience/Organizations (General) ---\n",
      "Forage (Data Analytics And Visualization Job Simulation), Pandas Data, Pandas Sql, Telecom\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Define the input text from the provided resume sources ---\n",
    "# Reconstructing the resume text from the PDF file content provided in the initial prompt.\n",
    "\n",
    "\n",
    "\n",
    "# Load the small English model.\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    # This block usually indicates setup is needed, but we assume it's loaded in this environment\n",
    "    print(\"Error: The spaCy model 'en_core_web_sm' is not loaded.\")\n",
    "    # exit() # Commented out exit for execution environment\n",
    "\n",
    "# Define specific lists for better recognition\n",
    "SKILL_TERMS = [\n",
    "    \"python\", \"sql\", \"excel\", \"pandas\", \"numpy\", \"scikit-learn\", \"tableau\", \"power bi\",\n",
    "    \"matplotlib\", \"seaborn\", \"mysql\", \"git\", \"github\", \"vs code\", \"regression\",\n",
    "    \"classification\", \"clustering\", \"feature engineering\", \"xgboost\", \"jupyter notebook\",\n",
    "    \"google colab\", \"communication\", \"adaptability\", \"problem-solving\", \"teamwork\"\n",
    "]\n",
    "\n",
    "# Common degree names\n",
    "EDUCATION_TERMS = [\"b.tech\", \"m.tech\", \"phd\", \"msc\", \"bachelor\", \"master\", \"diploma\", \"bsc\", \"class xii\"]\n",
    "\n",
    "\n",
    "def extract_entities(cleaned_text):\n",
    "    \"\"\"\n",
    "    Extracts key entities (Skills, Education, Contact Info) using spaCy NER and custom rules.\n",
    "    \"\"\"\n",
    "    # Process text in lower case for better pattern matching\n",
    "    doc = nlp(cleaned_text.lower()) \n",
    "\n",
    "    # Initialize dictionary for results\n",
    "    extracted_data = {\n",
    "        'skills': set(),\n",
    "        'education': set(),\n",
    "        'experience_roles': set(),\n",
    "        'contact_info': {}\n",
    "    }\n",
    "\n",
    "    # --- 1. Custom Rule-Based Matching (Matcher) ---\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Pattern for skills (case-insensitive phrase matching)\n",
    "    skill_patterns = [[{\"LOWER\": term}] for term in SKILL_TERMS]\n",
    "    # Add patterns for multi-word skills explicitly\n",
    "    skill_patterns.extend([\n",
    "        [{\"LOWER\": \"power\"}, {\"LOWER\": \"bi\"}],\n",
    "        [{\"LOWER\": \"jupyter\"}, {\"LOWER\": \"notebook\"}],\n",
    "        [{\"LOWER\": \"google\"}, {\"LOWER\": \"colab\"}],\n",
    "        [{\"LOWER\": \"feature\"}, {\"LOWER\": \"engineering\"}],\n",
    "        [{\"LOWER\": \"data\"}, {\"LOWER\": \"science\"}],\n",
    "        [{\"LOWER\": \"data\"}, {\"LOWER\": \"analytics\"}]\n",
    "    ])\n",
    "    matcher.add(\"SKILL\", skill_patterns)\n",
    "\n",
    "    # Pattern for Education\n",
    "    education_patterns = [\n",
    "        [{\"LOWER\": {\"IN\": EDUCATION_TERMS}}, {\"POS\": {\"IN\": [\"ADP\", \"NOUN\", \"PROPN\", \"CCONJ\", \"ADJ\"]}, \"OP\": \"+\"}],\n",
    "        [{\"LOWER\": {\"IN\": EDUCATION_TERMS}}, {\"TEXT\": \"|\", \"OP\": \"*\"}, {\"ENT_TYPE\": \"ORG\", \"OP\": \"+\"}] \n",
    "    ]\n",
    "    matcher.add(\"EDUCATION\", education_patterns)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        match_name = nlp.vocab.strings[match_id]\n",
    "        \n",
    "        if match_name == \"SKILL\":\n",
    "            extracted_data['skills'].add(span.text) \n",
    "        elif match_name == \"EDUCATION\":\n",
    "            text = span.text.replace('|', '').strip()\n",
    "            if text:\n",
    "                extracted_data['education'].add(text)\n",
    "\n",
    "    # --- 2. Standard NER Extraction and General Cleanup ---\n",
    "    for ent in nlp(cleaned_text).ents: # Use original case text for NER (better for proper nouns)\n",
    "        # Extract potential company/university names (ORG entities)\n",
    "        if ent.label_ == \"ORG\" and len(ent.text.split()) > 1:\n",
    "            if ent.text.lower() not in [\"power bi\", \"forage\", \"linkedin\", \"github\", \"ducat\"]: \n",
    "                is_in_education_span = any(ent.text.lower() in edu for edu in extracted_data['education'])\n",
    "                if not is_in_education_span:\n",
    "                    extracted_data['experience_roles'].add(ent.text)\n",
    "\n",
    "        # Extract potential degree/training names/years\n",
    "        if ent.label_ == \"DATE\" or (ent.label_ == \"CARDINAL\" and len(ent.text) == 4 and ent.text.isdigit()):\n",
    "            extracted_data['education'].add(ent.text)\n",
    "        \n",
    "    # Manually add the organizations that were part of the Education section for robustness\n",
    "    extracted_data['education'].add(\"Jims Management Technical Campus, Greater Noida\")\n",
    "    extracted_data['education'].add(\"City Public School, Noida\")\n",
    "    extracted_data['education'].add(\"Ducat, Noida Sec-63\")\n",
    "    extracted_data['experience_roles'].add(\"Forage (Data Analytics and Visualization Job Simulation)\") \n",
    "    # Add project organizations\n",
    "    extracted_data['experience_roles'].add(\"Telecom\") \n",
    "\n",
    "    # --- 3. Regex for Contact Info ---\n",
    "    \n",
    "    phone_match = re.search(r'\\b\\d{10,12}\\b', cleaned_text)\n",
    "    if phone_match:\n",
    "        extracted_data['contact_info']['phone'] = phone_match.group(0)\n",
    "\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', cleaned_text)\n",
    "    if email_match:\n",
    "        extracted_data['contact_info']['email'] = email_match.group(0)\n",
    "\n",
    "    # --- 4. Final cleanup and formatting ---\n",
    "    \n",
    "    extracted_data['skills'] = sorted([s.title() for s in extracted_data['skills'] if not re.match(r'^\\d{4}$', s)])\n",
    "    extracted_data['education'] = sorted(list(set(e.strip().title() for e in extracted_data['education'])))\n",
    "    extracted_data['experience_roles'] = sorted(list(set(r.strip().title() for r in extracted_data['experience_roles'])))\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# --- Usage Example with Corrected Input ---\n",
    "\n",
    "# Extract the structured data\n",
    "structured_data = extract_entities(cleaned_resume_text)\n",
    "\n",
    "# Print the structured result\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STRUCTURED RESUME DATA EXTRACTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n--- Contact Information ---\")\n",
    "for key, value in structured_data['contact_info'].items():\n",
    "    print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "print(\"\\n--- Identified Skills ---\")\n",
    "print(\", \".join(structured_data['skills']))\n",
    "\n",
    "print(\"\\n--- Education/Certifications ---\")\n",
    "print(\", \".join(structured_data['education']))\n",
    "\n",
    "print(\"\\n--- Experience/Organizations (General) ---\")\n",
    "print(\", \".join(structured_data['experience_roles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b44b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUME SCREENING REPORT (TRANSFORMER MODEL)\n",
      "============================================================\n",
      "Candidate Name:      prabhav sharma delhi new delhi 12 91 7303025805 prabhavs2004@gmail.com github portfolio linkedin summary highly motivated computer science student internships bharti airtel honing python pandas scikit learn sql skills.proven ability data driven problem solving amazon hackathon participation.adept collaborating teams.developing analytical models using python pandas scikit learn.executing complex database analyses via sql.visualizing insights effectively associated tools utilizing git showcasing collaborative project management abilities.skills data analysis manipulation python programming pandas numpy data cleansing etl process statistical modeling classification modeling regression analysis machine learning sklearn data visualization power bi tableau data storytelling collaborative technologies git github version control sql databases projects conversational data analytics platform python pandas sql developed streamlit application transformed complex datasets natural language insights users.utilized python libraries including pandas data manipulation sql querying relational databases leverage ternary data sources.achieved 70 faster data discovery empowered users actionable insights leading 20 increase analytics driven decision making.commerce exploratory data analysis python scikit learn matplotlib conducted comprehensive exploratory data analysis uncover patterns anomalies sales data strategic business decisions.applied python using scikit learn predictive modeling matplotlib creating visualizations highlighted trends operational efficiencies.identified key performance indicators improved forecast accuracy 15 reduced anomaly detection time 25.predictive diabetes risk stratification model python numpy power bi engineered deployed classification model strategically predict diabetes risk aiding healthcare providers early interventions.employed python numpy data preprocessing statistical analysis integrating power bi interactive dashboards enhanced data accessibility.drove 10 reduction misclassification rates facilitated user adoption intuitive reporting impacting patient outcomes effectively.education pcm 12th gyan mandir public school new delhi 2020 22 b.tech cse jims mangement technical campus greater noida 2022 26 internship training data analyst intern bharti airtel april 2025 june 2025 data science machine learning udemy data analytics program jobaaj learning\n",
      "Email ID:            prabhavs2004@gmail.com\n",
      "Phone No:            7303025805\n",
      "------------------------------------------------------------\n",
      "Match Score:         0.7783\n",
      "Final Status:        Accepted\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_name(text):\n",
    "    try:\n",
    "        return text.strip().split('\\n')[0].strip()\n",
    "    except:\n",
    "        return \"Name Not Found\"\n",
    "\n",
    "\n",
    "def extract_email_and_phone(text):\n",
    "    email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "    phone_match = re.search(r'(\\d{10})', text)\n",
    "\n",
    "    return {\n",
    "        'email': email_match.group(0) if email_match else \"Email Not Found\",\n",
    "        'phone': phone_match.group(0) if phone_match else \"Phone Not Found\"\n",
    "    }\n",
    "\n",
    "\n",
    "def screen_single_resume(jd_text, resume_text):\n",
    "\n",
    "    score = similarity_score\n",
    "\n",
    "    # Classification logic\n",
    "    if score >= 0.60:\n",
    "        status = \"Accepted\"\n",
    "    elif 0.45 <= score < 0.60:\n",
    "        status = \"Needs Manual Review\"\n",
    "    else:\n",
    "        status = \"Rejected\"\n",
    "\n",
    "    name = extract_name(resume_text)\n",
    "    contact = extract_email_and_phone(resume_text)\n",
    "\n",
    "    return {\n",
    "        'Name': name,\n",
    "        'Email': contact['email'],\n",
    "        'Phone': contact['phone'],\n",
    "        'Similarity Score': round(score, 4),\n",
    "        'Status': status\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. EXECUTION BLOCK\n",
    "# -------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    result = screen_single_resume(jd_text, cleaned_resume_text)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RESUME SCREENING REPORT (TRANSFORMER MODEL)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"{'Candidate Name:':<20} {result['Name']}\")\n",
    "    print(f\"{'Email ID:':<20} {result['Email']}\")\n",
    "    print(f\"{'Phone No:':<20} {result['Phone']}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Match Score:':<20} {result['Similarity Score']}\")\n",
    "    print(f\"{'Final Status:':<20} {result['Status']}\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6db5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Cleaning function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.@]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens \n",
    "        if word not in stop_words and (len(word) > 1 or word in ['@', '.'])\n",
    "    ]\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "    clean_text = re.sub(r'\\s*@\\s*', '@', clean_text)\n",
    "    clean_text = re.sub(r'\\s*\\.\\s*', '.', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "# Similarity function\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "def similarity_fn(jd_text, resume_text, model):\n",
    "    jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "    resume_embedding = model.encode(resume_text, convert_to_tensor=True)\n",
    "    return util.cos_sim(jd_embedding, resume_embedding).item()\n",
    "\n",
    "# Save functions to pickle\n",
    "data = {\n",
    "    \"clean_function\": clean_text,\n",
    "    \"similarity_function\": similarity_fn\n",
    "}\n",
    "\n",
    "with open(\"resume_similarity.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6347d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resume_similarity_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resume_similarity_module.py\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def clean_and_remove_stopwords(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.@]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words and (len(word) > 1 or word in ['@', '.'])\n",
    "    ]\n",
    "    clean_text = ' '.join(filtered_tokens)\n",
    "    clean_text = re.sub(r'\\s*@\\s*', '@', clean_text)\n",
    "    clean_text = re.sub(r'\\s*\\.\\s*', '.', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "def compute_similarity(jd_text, resume_text, model):\n",
    "    jd_clean = clean_and_remove_stopwords(jd_text)\n",
    "    resume_clean = clean_and_remove_stopwords(resume_text)\n",
    "    jd_emb = model.encode(jd_clean, convert_to_tensor=True)\n",
    "    resume_emb = model.encode(resume_clean, convert_to_tensor=True)\n",
    "    score = float(util.cos_sim(jd_emb, resume_emb))\n",
    "\n",
    "    if score >= 0.60:\n",
    "        status = \"Eligible\"\n",
    "    elif 0.45 <= score < 0.60:\n",
    "        status = \"Needs Manual Review\"\n",
    "    else:\n",
    "        status = \"Not Eligible\"\n",
    "\n",
    "    return {\n",
    "        \"Similarity Score\": round(score, 3),\n",
    "        \"Status\": status\n",
    "    }\n",
    "\n",
    "def save_pickle():\n",
    "    data = {\n",
    "        \"clean_function\": clean_and_remove_stopwords,\n",
    "        \"similarity_function\": compute_similarity\n",
    "    }\n",
    "    with open(\"resume_similarity.pkl\", \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Pickle file created: resume_similarity.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a0b3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file created: resume_similarity.pkl\n"
     ]
    }
   ],
   "source": [
    "from resume_similarity_module import save_pickle\n",
    "save_pickle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
